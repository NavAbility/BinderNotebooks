{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Gaussian Measurements Tutorial 2\n",
    "\n",
    "## Overview\n",
    "\n",
    "There are many, many ways in which non-Gaussian / ambiguous measurements occur, this tutorial will simply pick one common example to illustrate the basic concepts.  We ground the tutorial in an example: imagine a wheeled robot travelling along in a straight line, using wheel encoders to estimate the distance travelled.  There is, however, a quirk, from time to time the robot gets stuck on something and the wheel's spin for a few rotations before progress continues.  We assume that our software is able to detect when the wheels are slipping, but the process is only 40% accurate.  In this tutorial we will build a basic robot localization process that can handle \"bad\" measurement data.  We restrict this to a one dimensional example with only a few variables and factors, to help familiarize yourself with multi-modal beliefs.  \n",
    "\n",
    "This example shows one of four mechanisms by which non-Gaussian behavior can get introduced into a factor graph solution, see other tutorials for other mechanisms.  We will build a factor graph that contains multi-modal belief as well as `Uniform` and `Rayleigh` distributions, to help familiarize non-Gaussian measurements.   The ambiguous measurement example shown in this tutorial can readily be incorporated in other use cases, and also illustrates how consensus can occur when more information is included -- i.e. reducing uncertainties to unimodal marginal beliefs on each variable.\n",
    "\n",
    "This tutorial introduces non-Gaussian behavior through non-Gaussian measurements in the factor graph.  This tutorial shows how measurements do not have to follow a unimodal bell curve (i.e. normal/Gaussian), but can instead introduce non-Gaussian beliefs and let the joint inference find the best marginal posterior estimates on each of the desired variables in the factor graph.  The tutorial shows a multi-modal uncertainty can be introduced from non-Gaussian measurements, and then transmitted through the system.  \n",
    "\n",
    "\n",
    "### Signatures Used\n",
    "\n",
    "`ContinuousScalar`, `Prior`, `LinearRelative`, `Mixture`, `Normal`, `plotBelief`, `plotDFG`, `isInitialized`, `initAll!`, `solveGraph!`\n",
    "\n",
    "## Ambiguous Data Example\n",
    "\n",
    "To start, the two major mathematical packages are brought into scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages if not included in your environment by default for whatever reason.\n",
    "import Pkg; [Pkg.add(s) for s in [\"NavAbilitySDK\",\"Logging\"]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress unnecessary printouts in the notebook\n",
    "using Logging\n",
    "Logging.disable_logging(Logging.Warn)\n",
    "\n",
    "using NavAbilitySDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Starting a 1D Factor Graph\n",
    "\n",
    "### Variable, `ContinuousScalar`\n",
    "\n",
    "The first thing to do is setup a client-context to talk with the NavAbility Platform.\n",
    "The next step is to describe the inference problem with a graphical model by populating the factor graph with variable nodes.\n",
    "The variable nodes are identified by `String`s, namely `x0, x1, x2, x3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need a unique userId:robotId, and can keep using that across all tutorials\n",
    "userId = \"Guest\"\n",
    "robotId = \"SDKjl_\"*(string(uuid4())[1:4])\n",
    "\n",
    "# also create a client connection\n",
    "client = NavAbilityHttpsClient()\n",
    "\n",
    "# You'll need a unique session number each time you run a new graph\n",
    "sessionId = \"Tutorial2_\"*(string(uuid4())[1:4])\n",
    "# context is the object to use below\n",
    "context = Client(userId,robotId,sessionId)\n",
    "\n",
    "# let's collect all the async responses and wait at the end\n",
    "resultIds = Task[]\n",
    "# add the first node\n",
    "# addVariable and keep the transaction ID\n",
    "push!(resultIds, \n",
    "  addVariable(client, context, Variable(\"x0\", :ContinuousScalar))\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Factor, (Euclidean(1))\n",
    "\n",
    "Factor graphs are bipartite graphs with `factors` that act as mathematical structure between interacting `variables`.\n",
    "After adding node `x0`, a singleton factor of type `Prior` (which was defined by the user earlier) is 'connected to' variable node `x0`.\n",
    "This unary factor is taken as a `Distributions.Normal` distribution with zero mean and a standard deviation of `1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is unary (prior) factor and does not immediately trigger autoinit of :x0.\n",
    "f = Factor(\"x0f1\", \"Prior\", [\"x0\"], PriorData(Z=Normal(0,1)))\n",
    "\n",
    "push!(resultIds, addFactor(client, context, f));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**, this example is using just `Prior`, which is a simplification for the sake of this tutorial.  Look out for on-manifold prior factors in other code examples.\n",
    "\n",
    "### Visualizing Graph Topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As with tutorials 1 we can visualize the factor graph in the NavAbility App as we add variables.\n",
    "println(\"https://app.navability.io/cloud/graph/?userId=$userId&robotStartsWith=$robotId&sessionStartsWith=$sessionId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topological graph plot above shows the two node factor graph, one variable and one prior factor.  This example uses graph-based automatic variable initialization which was discussed in ICRA 2022 Tutorial 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Variable and Relative Factor\n",
    "\n",
    "Now let's add a second variable `x1`, and connect it to `x0` with a `LinearRelative` factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's wait to make sure all nodes were added\n",
    "waitForCompletion(client, resultIds; expectedStatuses=[\"Complete\"])\n",
    "\n",
    "# add x1\n",
    "push!(resultIds, \n",
    "  addVariable(client, context, Variable(\"x1\", :ContinuousScalar))\n",
    ")\n",
    "\n",
    "# P(Z | :x1 - :x0 ) where Z ~ Normal(10,1)\n",
    "f = Factor(\"x0x1f1\", \"LinearRelative\", [\"x0\",\"x1\"], LinearRelativeData(Z=Normal(10.0,1)))\n",
    "push!(resultIds, addFactor(client, context, f));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Variable Probability Belief\n",
    "\n",
    "The `RoMEPlotting.jl` package allows visualization (plotting) of the belief state over any of the variable nodes.\n",
    "Remember the first time executions are slow given the required code compilation, and that future versions of these packages will use more precompilation to reduce first execution running cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# allow the first run some time to complete Julia JIT compiling of plottinng functions.\n",
    "\n",
    "println(\"https://app.navability.io/cloud/map/?userStartsWith=$userId&robotStartsWith=$robotId&sessionStartsWith=$sessionId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By forcing the initialization of `x1` and plotting its belief estimate,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push!(resultIds,\n",
    "  solveSession(client, context)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotBelief(fg, [:x0, :x1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the predicted influence of the `P(Z| X1 - X0) = LinearRelative(Normal(10, 1))` is shown by the red trace.\n",
    "The red trace (predicted belief of `x1`) is nothing more than the approximated convolution of the current marginal belief of `x0` with the conditional belief described by `P(Z | X1 - X0)`.\n",
    "\n",
    "\n",
    "### Mixture Distribution on Next Relative Factor (Wheel Slip)\n",
    "\n",
    "As the robot continues to drive from `x1` to `x2`, the robot software detects wheel slip has occurred.  Here then is the critical point, how should the next odometry measurement incorporate this ambiguous information.  First, we add the next `ContinuousScalar` variable `x2` as before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push!(resultIds, \n",
    "  addVariable(client, context, Variable(\"x2\", :ContinuousScalar))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, however, the odometry factor from `x1` uses a more complicated `Mixture` likelihood function.  Given a 40% accuracy in wheel slip detection, we are going to construct a `Mixture` distribution as the measurement, where 40% weight goes to what we think the distance travelled is while the remaining 60% weight is designated to 'faulty' odometry measurement.  \n",
    "\n",
    "Taking the full encoder turns as 60 units distance, with slip being detected on and off throughout, we estimate the majority slip case as a Rayleigh distribution from 0.  Since we know very little about the non-slip case, let's take the robot distance traveled as uniformly distributed somewhere between 40 up to 60 units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = MixtureData(LinearRelativeData, (slip=Rayleigh(5), noslip=Uniform(35, 60)), [0.4, 0.6], 2)\n",
    "mmo = Factor(\"x1x2f1\", \"LinearRelative\", [\"x1\",\"x2\"], md)\n",
    "push!(resultIds, addFactor(client, context, mmo));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `mmo` variable illustrates how a near arbitrary mixture probability distribution can be used as a conditional relationship between variable nodes in the factor graph.\n",
    "In this case, a 40%/60% balance of a Rayleigh and truncated Uniform distribution which acts as a multi-modal conditional belief.\n",
    "Interpret carefully what a conditional belief of this nature actually means.\n",
    "\n",
    "Following the tutorial's practical example frameworks (robot navigation or time travel), this multi-modal belief implies that moving from one of the probable locations in `x1` to a location in `x2` by some processes defined by `mmo=P(Z | X2, X1)` is uncertain to the same 40%/60% ratio.\n",
    "In practical terms, collapsing (through observation of an event) the probabilistic likelihoods of the transition from `x1` to `x2` may result in the `x2` location being at either 15-20, or 40-65-ish units.\n",
    "The predicted belief over `x2` is illustrated by plotting the predicted belief (green trace), after forcing initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push!(resultIds,\n",
    "  solveSession(client, context)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"https://app.navability.io/cloud/map/?userStartsWith=$userId&robotStartsWith=$robotId&sessionStartsWith=$sessionId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the multi-modal belief in the marginal posterior belief of `x3`.  Also note that Kernel Density Estimate methods can appear to produce noisy belief density estimates.  While this does sometimes occur, it is harmless and has no impact on the accuracy of the mean point estimate produced for each mode in the associated belief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Adding Variable `x3`\n",
    "\n",
    "In suspecting that the robot did get stuck and had its wheels slip, we reverse the robot 50 units and add pose variable `x3`,  i.e. a factor measurement `LinearRelative(Normal(-50,1))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "push!(resultIds, \n",
    "addVariable(client, context, Variable(\"x3\", :ContinuousScalar))\n",
    ")\n",
    "\n",
    "f = Factor(\"x2x3f1\", \"LinearRelative\", [\"x2\",\"x3\"], LinearRelativeData(Z=Normal(-50,1)))\n",
    "push!(resultIds, addFactor(client, context, f));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expands the factor graph to four variables and four factors.\n",
    "This part of the tutorial shows how a unimodal likelihood (conditional belief) can transmit the bimodal belief currently contained in `x2`.  By solving the graph, we get numerical estimates for the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push!(resultIds,\n",
    "  solveSession(client, context)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the plotting the marginal posterior belief estimates over each variable to see what the position estimates are given available info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO plotBelief(fg, [:x0, :x1, :x2, :x3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the blue trace (`x3`) is a shifted and slightly spread out version of the initialized belief on `x2`, through the convolution with the conditional belief `P(Z | X2, X3)`.\n",
    "\n",
    "### The Last Factor\n",
    "\n",
    "\n",
    "Global inference over the entire factor graph has still not occurred, and will at this stage produce roughly similar results to the predicted beliefs shown above.\n",
    "Only by introducing more information into the factor graph can inference extract more precise marginal belief estimates for each of the variables.\n",
    "A final piece of information added to this graph is a factor directly relating `x3` with `x0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Factor(\"x3x0f1\", \"LinearRelative\", [\"x3\",\"x0\"], LinearRelativeData(Z=Normal(30,1)))\n",
    "push!(resultIds, addFactor(client, context, f));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay close attention to what this last factor means in terms of the probability density traces shown in the previous figure.\n",
    "The blue trace for `x3` has two major modes, one that overlaps with `x0, x1` near 0 and a second mode further to the left at -40.\n",
    "The last factor introduces a shift `LinearRelative(Normal(40,1))` which essentially aligns the left most mode of `x3` back onto `x0`.\n",
    "\n",
    "This last factor forces a mode selection through consensus.\n",
    "By doing global inference, the new information obtained in `x3` will be equally propagated to `x2` where only one of the two modes will remain.\n",
    "\n",
    "## Solve the Graph\n",
    "\n",
    "\n",
    "Global inference is achieved with local computation using two function calls, as follows.  In this solve we show that a Bayes tree structure is returned by the solver.  More information about the Bayes tree is linked below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push!(resultIds,\n",
    "  solveSession(client, context)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the visualization\n",
    "#TODO plotBelief(fg, [:x0, :x1, :x2, :x3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting posterior marginal beliefs over all the system variables are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFG.getPPESuggested.(fg, [:x0;:x1;:x2;:x3])\n",
    "\n",
    "vars = getVariables(client, context; detail=SUMMARY) |> fetch\n",
    "lbls = vars .|> x->x[\"label\"]\n",
    "ppes = vars .|> x->float.(x[\"ppes\"][1][\"suggested\"])\n",
    "\n",
    "println.(lbls .=> ppes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Wheels Did Slip!\n",
    "\n",
    "Look, the resulting distance from `x1` to `x2` with all the data available turns out to be 10 units -- much less than the 60 units measured by the encoder, coupled with the on and off slip detection.  Our example here is constructed in such a way that when all data is considered together a clear answer can be extracted.  In real situations, the beliefs could be more nuanced -- which is all the more reason to consider non-Gaussian estimation which permanently keeps track of the nuanced features hidden in the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It is important to note that although this tutorial ends with all marginal beliefs having near Gaussian shape and are unimodal, that the package supports multi-modal belief estimates during both the prediction and global inference processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The next tutorial looks at weakly observable (a.k.a. missing / insufficient data, or underdetermined) situations where all Gaussian measurements result in highly non-Gaussian and multi-modal posterior estimates.  More information about the Bayes tree can be found in the [Caesar.jl source documentation](https://juliarobotics.org/Caesar.jl/latest/principles/bayestreePrinciples/).\n",
    "\n",
    "### Case Study: Marine Surface Navigation\n",
    "\n",
    "Furthermore, this tutorial has practical value.  For example, NavAbility developed a GPS-denied navigation technique for marine surface vehicles whereby consecutive radar sweeps are correlated for extracting odometry measurements.  These correlations, as it turns out, are non-Gaussian and on many occasions and exhibit multi-modal behavior similar to the tutorial illustrated above.\n",
    "\n",
    "**Real World Non-Gaussian Odometry Measurement**\n",
    "\n",
    "The left image below shows two consecutive `360 degree` radar sweeps from a marine surface vehicle.  The right-hand image shows a slice from the dense correlation map on the `SpecialEuclidean(2)` manifold when looking for the best alignment between the two radar sweeps -- notice the non-Gaussian / multi-modal behavior!  This type of non-Gaussian measurement can readily be used in the NavAbility and Caesar.jl solver system.  See the [NavAbility Marine Surface Navigation Case Study page](https://www.navability.io/applications/marine/) for more details.\n",
    "\n",
    "![img](http://www.navability.io/wp-content/uploads/2022/04/MarineRadarAlignFigure-1024x485-1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
